\chapter{Alternative proof via slope approximation}

In this chapter I present an alternative proof of the de Moivre-Laplace theorem which approximates the slope of the probability mass function and the density function between the standardized binomial and the normal distribution. The idea of this proof comes from Prof. Peter Pickl and was first described in the thesis by ... \todo{cite}.

\section{Intuition behind the proof}

\section{Necessary prior knowledge from ODE}

In order to formalize the above proof sketch into a valid proof we will need some concepts and theorems from the theory about ordinary differential equations. An equation of the form

\begin{align} \label{ode:def}
  y'(x) = f(x,y(x))
\end{align}

with $f:\R^2 \to \R$ will be called an \emph{ordinary differential equation} (of first order)~\cite[p. 465]{stoer}\cite{wiki:ode}\footnote{We may also restrict the domain of $f$ to the set $I\times \Omega$ with $I$ being an interval and $\Omega\subseteq \R$\todo{cite}. For this chapter the more special definition is sufficient.} Instead of the term ``ordinary differential equation'' also its acronym ``ODE'' is often used~\cite[p. 2]{ricardo}\cite{wiki:ode}. Any function $y:\R\to\R$ fulfilling the ODE \eqref{ode:def} is called a \emph{solution of the ordinary differential equation}~\cite[p. 8]{ricardo}\cite{wiki:ode}. For example the density function $\fphi{x} = \frac{1}{\sqrt{2\pi}} \fexp{-\frac{x^2}2}$ of the normal distribution is the solution of the ODE

\begin{align}
  y'(x) = -xy(x)
\end{align}

with the initial value $y(0) = \frac{1}{\sqrt{2\pi}}$ \todo{cite}. In general the ordinary differential equation with initial value

\begin{align}
  y'(x) = -xy(x),\ y(0)=a
\end{align}

is solved by $y_a(x) = a \fexp{-\frac{x^2}2}$. ODEs can be approximated numerically with \emph{one-step methods}~\cite[pp.~471~ff]{stoer}. A one-step method is described by a function $M(x,y,h,f)$ with a step size $h>0$~\cite[p.~473]{stoer}. Given an initial value $y(x_0)=y_0$ the approximations $y_k$ at the points $x_k = x_0 + kh$ ($k\in\N$) can be obtained recursively via the relation

\begin{align}
  y_{k+1} = y_k + h M(x_k, y_k, h, f)
\end{align}

for $k \le 0$~\cite[p. 473]{stoer}. One of the most famous one-step method is the \emph{Euler method} with $M(x,y,h,f) = f(x,y)$~\cite[p. 473]{stoer}. We will use the concept of one-step methods to connect the density function of the normal distribution with the probability mass function of the binomial distribution. To state the theorem of the global error of one-step methods we will need the following definition of the local discretization error (see \cite[pp. 473-474]{stoer}):

\begin{definition}[Local discretization error]
  The \emph{local discretization error} is the error of the one-step method which is produced during a step of approximation. Given an one-step method $M(x,y,h,f)$ its local discretization error at the point $x_k$ is the value

  \begin{align}
    \tau(x_k) = \frac{y(x_{k+1})-y(x_k)}h - M(x_k,y(x_k),h,f)
  \end{align}
\end{definition}

\noindent Therefore $\tau$ fulfills:

\begin{align}
  \underbrace{y(x_{k+1})}_{\text{exact solution}} = \underbrace{y(x_k) + M(x_k,y(x_k),h,f)h}_{\text{approximation of one-step method}} + \underbrace{\tau(x_k)h}_{\text{error}}
\end{align}

\noindent The error of an one-step method can be estimated as following~\cite[pp. 478-479]{stoer}:

\begin{theorem}[Global error of one-step method] \label{thm:ode_global_error}
  Given an ODE with initial value

  \begin{align}
    y'(x)=f(x,y(x)),\ y'(x_0)=y_0
  \end{align}

  which has the exact solution $y(x)$. Let $M(x,y,h,f)$ be a continuous one-step method for this ODE with step size $h > 0$. Let $M(x,y,h,f)$ fulfill the Lipschitz condition\footnote{See \cite[p. 467]{stoer} and \cite{wiki:lipschitz} for more details about Lipschitz continuity.} regarding the variable $y$ with the Lipschitz constant $L > 0$, i.e. for all $x,y_1,y_2\in\R$ we have
  \begin{align}
    \abs{M(x,y_1,h,f)-M(x,y_2,h,f)} \le L \abs{y_1-y_2}
  \end{align}
  Let the absolute value of the local discretization error be bounded by $C h^p$ for a $C > 0$ and $p\in \N$:
  \begin{align}
    \abs{\tau(x_k)} = \abs{\frac{y(x_{k+1})-y(x_k)}h - M(x_k,y(x_k),h,f)} \le Ch^p
  \end{align}
  Then the global error $\abs{y(x_k)-y_k}$ is bounded by $\frac 1L \br{\fe{L\abs{x_k-x_0}}-1} C h^p$:
  \begin{align}
    \abs{y(x_k)-y_k} \le \frac{\fe{L\abs{x_k-x_0}}-1}{L} \cdot Ch^p
  \end{align}
\end{theorem}
\todo{premises all right?!}

We may also have an one-step method backwards by setting $h < 0$. The above definitions and the  above theorem \ref{thm:ode_global_error} are applicable in this case.

\section{Local version of de Moivre-Laplace}

