\chapter{Alternative proof via slope approximation}

In this chapter I present an alternative proof of the de Moivre-Laplace theorem which approximates the slope of the probability mass function and the density function between the standardized binomial and the normal distribution. The idea of this proof comes from Prof. Peter Pickl and was first described in the thesis ``Der Zentrale Grenzwertsatz'' by Karolina Fels~\cite{fels}.

\section{Intuition behind the proof}

In this section I want to present the proof idea of Karolina Fels \cite[pp. 16-19]{fels}. Thereby I will slightly change her proof sketch. She started with the function $f(x) = \binom nk p^kq^{n-k}$ with $k=np+x$ and approximated $f'(x)$ by calculating $f(x)-f(x-1)$~\cite[p. 16]{fels}. We will approximate the slope of $\bbs$ between $\bb{\x[k+1]}$ and $\bb{\x[k]}$:

\begin{align}
  \frac{\bb{\x[k+1]}-\bb{\x[k]}}h &= \frac 1h \br{\binom n{k+1}p^{k+1}q^{n-k-1} - \binom nk p^k q^{n-k}} \nl
  &= \frac 1h \binom nk p^k q^{n-k} \br{\frac{(n-k)p}{(k+1)q}-1} \nl
  &= \frac 1h \bb{\x[k]} \frac{np-kp-kq-q}{(k+1)q} \nl
  &= - \frac 1h \bb{\x[k]} \frac{k-np+q}{(k+1)q} \nl
  &= - \frac 1h \bb{\x[k]} \br{\frac{k-np}{npq} + \frac{q}{npq}} \frac{np}{k+1} \nl
  &= - \frac 1h \bb{\x[k]} \br{\x[k]h + q h^2} \frac{np}{k+1} \nl
  &= -\x[k] \bb{\x[k]} \frac{np}{k+1} + \bb{\x[k]} q h \frac{np}{k+1}
\end{align}

Let us assume that the cumulative distribution of $\BBs$ can be approximated by a distribution with density function $\phi(x)$ for $n\to\infty$. Under the limit $n\to\infty$ we have $h\to 0$. For a fixed $x\in\R$ we also set $\x = \xrnd{x} = \frac{\rnd{np+ x \sqrt{npq}}-np}{\sqrt{npq}}$ which is the closest $\x$ to $x$. We get
\begin{align}
  \frac{\bb{\x[k+1]}-\bb{\x[k]}}h & \to \phi'(x) \nl
  \bb{\x[k]} \to \fphi{x} \nl
  \frac{np}{k+1} &= \frac{np}{\rnd{np+x\sqrt{np}}+1} \to 1
\end{align}
Therefor $\fphi{x}$ is described by the ODE
\begin{align}
  \phi'(x) = -x\phi(x)
\end{align}

This ODE is solved by $\phi_c(x) = c \fexp{-\frac{x^2}2}$. Because $\int_\R \fphi{1} \d{t} = 1$ and $\int_\R \fexp{-\frac{t^2}2} d{t} = \sqrt{2\pi}$~\cite[p. 47]{georgii} we get finally $\fphi{x} = \frac{1}{\sqrt{2\pi}} \fexp{-\frac{x^2}2}$.

\section{Necessary prior knowledge from ODE}

In order to formalize the above proof sketch into a valid proof we will need some concepts and theorems from the theory about ordinary differential equations. An equation of the form

\begin{align} \label{ode:def}
  y'(x) = f(x,y(x))
\end{align}

with $f:I \times \R \to \R$ and $I$ being a connected subset of $\R$ will be called an \emph{ordinary differential equation} (of first order)~\cite[p. 465]{stoer}\cite{wiki:ode}. Instead of the term ``ordinary differential equation'' also its acronym ``ODE'' is often used~\cite[p. 2]{ricardo}\cite{wiki:ode}. Any function $y:\R\to\R$ fulfilling the ODE \eqref{ode:def} is called a \emph{solution of the ordinary differential equation}~\cite[p. 8]{ricardo}\cite{wiki:ode}. For example the density function $\fphi{x} = \frac{1}{\sqrt{2\pi}} \fexp{-\frac{x^2}2}$ of the normal distribution is the solution of the ODE
\begin{align}
  y'(x) = -xy(x)
\end{align}
with the initial value $y(0) = \frac{1}{\sqrt{2\pi}}$. This follows from the theorem:

\begin{theorem} \label{chr_ode}
  The solution to the ODE $y'(x) = -xy(x)$ with the initial value $y(a)=b$ for $a\in\R$ and $b\in \R\setminus\{0\}$ is
  \begin{align}
    y(x) = b \fexp{-\frac{x^2}2+\frac{a^2}2} = \frac{b}{\fphi{a}} \fphi{x}
  \end{align}
\end{theorem}

\begin{proof}
  The ODE $y'(x)=-xy(x)$ can be solved by separation of variables (see \cite[p. 28]{ricardo} and \cite{wiki:separation}). We can derive:

  \begin{align}
      && y'(x)& =-xy(x) \bnl
      &\implies & \int_{y(a)}^{y(x)} \frac 1y \d{y} & = \int_a^x -t \d{t} \bnl
      &\implies & \left[\ln{y}\right]^{y(x)}_{y(a)=b} & = \left[-\frac{t^2}2\right]_a^x \bnl
      &\implies & \ln(y(x))-\ln(b) &= -\frac{t^2}2 + \frac{a^2}2 \bnl
      &\implies & y(x) &= b \fexp{-\frac{x^2}2 + \frac{a^2}2} = \frac b{\fphi{a}}\fphi{x}
  \end{align}
\end{proof}

ODEs can be approximated numerically with \emph{one-step methods}~\cite[pp.~471~ff]{stoer}. A one-step method is described by a function $M(x,y,h,f)$ with a step size $h>0$~\cite[p.~473]{stoer}. Given an initial value $y(x_0)=y_0$ the approximations $y_k$ at the points $x_k = x_0 + kh$ ($k\in\N$) can be obtained recursively via the relation

\begin{align}
  y_{k+1} = y_k + h M(x_k, y_k, h, f)
\end{align}

for $k \le 0$~\cite[p. 473]{stoer}. One of the most famous one-step method is the \emph{Euler method} with $M(x,y,h,f) = f(x,y)$~\cite[p. 473]{stoer}. We will use the concept of one-step methods to connect the density function of the normal distribution with the probability mass function of the binomial distribution. To state the theorem of the global error of one-step methods we will need the following definition of the local discretization error (see \cite[pp. 473-474]{stoer}):

\begin{definition}[Local discretization error]
  The \emph{local discretization error} is the error of the one-step method which is produced during a step of approximation. Given an one-step method $M(x,y,h,f)$ its local discretization error at the point $x_k$ is the value

  \begin{align}
    \tau(x_k) = \frac{y(x_{k+1})-y(x_k)}h - M(x_k,y(x_k),h,f)
  \end{align}
\end{definition}

\noindent Therefore $\tau$ fulfills:

\begin{align}
  \underbrace{y(x_{k+1})}_{\text{exact solution}} = \underbrace{y(x_k) + M(x_k,y(x_k),h,f)h}_{\text{approximation of one-step method}} + \underbrace{\tau(x_k)h}_{\text{error}}
\end{align}

\noindent The error of an one-step method can be estimated as~\cite[pp. 478-479]{stoer}:

\begin{theorem}[Global error of one-step method] \label{thm:ode_global_error}
  Let $f:I\times \R\to\R$ and let $y(x)$ be the solution to the following ODE with initial value

  \begin{align}
    y'(x)=f(x,y(x)),\ y'(x_0)=y_0,\ x_0 \in I
  \end{align}

  Let $M(x,y,h,f)$ be a continuous one-step method for this ODE with step size $h > 0$. Let $M(x,y,h,f)$ fulfill the Lipschitz condition\footnote{See \cite[p. 467]{stoer} and \cite{wiki:lipschitz} for more details about Lipschitz continuity.} regarding the variable $y$ with the Lipschitz constant $L > 0$, i.e. for all $x\in I$ and $y_1,y_2\in\R$ we find
  \begin{align}
    \abs{M(x,y_1,h,f)-M(x,y_2,h,f)} \le L \abs{y_1-y_2}
  \end{align}
  Let the absolute value of the local discretization error be bounded by $C h^p$ for a $C > 0$ and $p\in \N$, i.e. for all $x_k \in I$:
  \begin{align}
    \abs{\tau(x_k)} = \abs{\frac{y(x_{k+1})-y(x_k)}h - M(x_k,y(x_k),h,f)} \le Ch^p
  \end{align}
  Then the global error $\abs{y(x_k)-y_k}$ is bounded by $\frac 1L \br{\fe{L\abs{x_k-x_0}}-1} C h^p$:
  \begin{align}
    \abs{y(x_k)-y_k} \le \frac{\fe{L\abs{x_k-x_0}}-1}{L} \cdot Ch^p
  \end{align}
\end{theorem}

We may also have an one-step method backwards by setting $h < 0$. The above definitions and the  above theorem \ref{thm:ode_global_error} are applicable in this case, too.

\section{Local version of de Moivre-Laplace}

We will prove the local version of the de Moivre-Laplace theorem in the following form

\newcommand*{\bnp}{\bb{\xnp}}
\newcommand*{\knp}{m}
\newcommand*{\xnp}{\x[\knp]}
\newcommand*{\phin}[2][n]{y_{#1}\left({#2}\right)}
\newcommand*{\dphin}[2][n]{y_{#1}'\left({#2}\right)}

\begin{theorem}
  Let $I$ be a bounded interval and let $\xnp$ be the mode of $\bbs$ (i.e. the $\x$ where $\bb{\x}$ attains its maximum). Let $L = \sup\{\abs{x}: x \in I\}$. Then there is a $C > 0$ independent of $\x$ and $n$ such that

  \begin{align}
    \bb{\x} = \frac{\bnp}{\fphi{\xnp}}\fphi{\x} + \bigabs{{\bnp}\cdot \frac{\fe{L\abs{\x-\xnp}}-1}L Ch}
  \end{align}
\end{theorem}

\begin{remark} \label{remark:alt_local}
  The mode of $\xnp$ is the $\x[k]$ with $k = \lfloor (n+1)p \rfloor$ \cite{nicolas}\cite{wiki:binomial_distribution}. Thus we have
  \begin{align}
    np+p-1 \le \knp \le np+p \implies \abs{\knp-np} \le 1 \implies \abs{\xnp} \le h \implies \lim_{n\to\infty} \xnp = 0
  \end{align}
  We may also generalize the theorem to each bounded set $I \subset \R$. Because
  \begin{align}
    \abs{\x[k]-\xnp} \le \abs{\x[k]}+\abs{\xnp} \le L + h \le L + \frac{1}{\sqrt{pq}}
  \end{align}
  we can directly deduce that there is a $\tilde C > 0$ independent of $\x$ and $n$ with
  \begin{align}
    \bb{\x} \in \frac{\bnp}{\fphi{\xnp}}\fphi{\x} + \bigabs{{\bnp}\cdot \tilde Ch}
  \end{align}
  Please note that we get the right convergence order $\bigo{h^2}$ of the remaining error because $\bnp \in \bigo{h}$ as we will prove later. However the error estimation of this theorem is worse than in the standard proof with Stirling's formula. So we cannot use this kind of proof to get a better error estimation.
\end{remark}

This proof will heavily bases on the theory of one-step methods for approximating ordinary differential equations. Thereby we will revert the perspective: Whereas in the theory of ODEs one-step methods are approximations of unknown solutions of ODEs, we will consider the probability mass function of the binomial distribution as the result of an one-step method. This one-step method corresponds to an ODE and the solution to this ODE will be an approximation to the probability mass function. Thus we flip the role of the exact solution and the approximation. This makes this proof in particular interesting and my be a method which might be applicable in other proofs, too.

\begin{proof}
  We prove this theorem by extending $I$ to $\interval{-L}{L}$ \todo{richtig?} so that $\xnp \in I$. We start with investigating $\bb{\x[k+1]}-\bb{\x[k]}$ (let $\pol{\x}$ be the set of all polynomials in $\x$):

  \begin{align}
    \bb{\x[k+1]} - \bb{\x[k]} &= \binom n{k+1} p^{k+1}q^{n-k-1} - \binom nk p^k q^k \nl
    &= \binom nk p^k q^{n-k} \br{\frac{\binom n{k+1}p}{\binom nk q} -1} \nl
    &= \bb{\x[k]} \br{\frac{k! (n-k)! p}{(k+1)!(n-k-1)! q} -1} \nl
    &= \bb{\x[k]} \br{\frac{(n-k)p}{(k+1)q}-1} \nl
    &
    \begin{comment}
      k = np + \x[k]\sqrt{npq}
    \end{comment} \nl
    &= \bb{\x[k]} \br{\frac{npq-\x[k]p\sqrt{npq}}{npq + \x[k]q\sqrt{npq}+q}-1} \nl
    &= \bb{\x[k]} \br{\frac{1-\x[k]ph}{1+\x[k]qh+qh^2}-1} \nl
    &
    \begin{comment}
      \text{\todo{}} \frac1{1+x} = 1 - x+ \bigo{x^2}
    \end{comment} \nl
    &= \bb{\x[k]} \br{\br{1-\x[k]ph}\br{1+\x[k]qh + \bigo{\pol{x}h^2}}-1} \nl
    &= \bb{\x[k]} \br{-\x[k]h + \bigo{\pol{\x}h^2}} \nl
    &= -\x[k]\bb{\x}h +  \bigo{\bb{\x[k]}\pol{\x}h^2}
  \end{align}

  For any $\seq{\epsilon_n} \in \bigo{\bb{\x[k]}\pol{\x[k]}h^2}$ there is a polynomial $p(\x[k])$ and a $A_1 > 0$ with
  \begin{align}
    \abs{\epsilon_n} \le \bb{\x[k]} \abs{p(\x[k])} \cdot A_1 h^2
  \end{align}
  By the definition of $\xnp$ as the mode of $\bbs$ we can follow $\bb{\x[k]} \le \bnp$. The polynomial $p(\x)$ is bounded on $I$ too so that there is a $A_2>0$ independent of $\x$ with $\abs{p(x)} \le A_2$. We set $A=A_1\cdot A_2$ and we get $\abs{\epsilon_n} \le \bnp \cdot A h^2$ or $\seq{\epsilon_n}\in \bigabs{\bnp \cdot A h^2}$. So we can conclude

  \begin{align}
    \bb{\x[k+1]} &= \bb{\x[k]} -\x[k]\bb{\x[k]} h + \bigabs{\bnp \cdot A h^2}\bnl
    &
    \begin{comment}
      \text{Set } f(x,y) = -xy
    \end{comment} \bnl
    &= \bb{\x[k]} - \br{\f{\x[k], \bb{\x[k]}} + \bigabs{\bnp \cdot A h}}h \bnl
    &
    \begin{comment}
      \text{Set } M(x,y,h,f) = f(x,y) + \bigabs{\bnp \cdot A h}
    \end{comment} \bnl
    &= \bb{\x[k]} + M(\x[k], \bb{\x[k]}, f, h) h
  \end{align}

  Therefore we can think about $\bb{\x}$ as the results of the one-step method $M(x,y,f,h)=f(x,y)+\bigabs{\bnp \cdot A h}$ whereby we take $\bnp$ for $\xnp$ as the initial value. This one-step method corresponds to the ODE with initial condition

  \begin{align}
    \dphin{x}=f(x,\phin{x})=-x\phin{x},\ \phin{\xnp} = \bb{\xnp}
  \end{align}
  Its solution is (see theorem \ref{chr_ode})
  \begin{align}
    \phin{x} = \bb{\xnp}\fexp{-\frac{x^2}2+\frac{\xnp^2}2} = \frac{\bb{\xnp}}{\fphi{\xnp}} \fphi{x}
  \end{align}
  We find by using the Taylor series
  \begin{align}
    & \phin{x+h} = \phin{x} -x\phin{x}h+\br{\tilde x^2-1}\phin{\tilde x} h^2
  \end{align}
  whereas $\tilde x$ is a number in the interval bounded by $x$ and $x+h$. Because $\phin{\tilde x} = \frac{\bnp}{\fphi{\xnp}} \fphi{\tilde x}$ and $\frac{1}{\fphi{\xnp}} \br{\tilde x^2-1} \fphi{\tilde x}$ is bounded on $I$ there is a $B > 0$ with $\abs{\tilde x^2-1} \phin{\tilde x} h^2 \le \bnp \cdot B h^2$ independent of $\x[k]$. Thus

  \begin{align}
    \frac{\phin{x+h}-\phin{x}}h &= -x\phin{x}+\br{\tilde x^2-1}\phin{x} h \in -x\phin{x} + \bigabs{\bnp \cdot Bh}
  \end{align}
  Now we can estimate the local discretization error
  \begin{align}
    \abs{\tau(\x)} &= \abs{\frac{\phin{\x[k]+h}-\phin{\x}}h -  M(\x,\phin{\x},f,h)} \nl
    &\in \abs{-\x\phin{\x} + \bigabs{\bnp \cdot Ah} +\x\phin{\x} - \bigabs{\bnp \cdot Bh}} \nl
    &= \abs{\bigabs{\bnp \cdot Ah}-\bigabs{\bnp \cdot Bh}} \nl
    &\subseteq \bigabs{\bnp \cdot (A+B)h} \nl
    &
    \begin{comment}
      \text{Set } C=A+B
    \end{comment} \nl
    &= \bigabs{\bnp \cdot Ch}
  \end{align}
  from which follows $\abs{\tau(\x)} \le \bnp \cdot Ch$. Besides approximating $\abs{\tau(x)}$ we also need to prove, that $f(x,y)$ is Lipschitz continuous regarding $y$. We have
  \begin{align}
    \abs{f(x,y_1)-f(x,y_2)} &= \abs{-xy_1+xy_2} = \abs{x} \cdot \abs{y_1-y_2} \le L \abs{y_1-y_2}
  \end{align}

  \noindent From the theorem \ref{thm:ode_global_error} about the global error of one-step methods we can conclude
  \begin{align}
    & \abs{\bb{\x}-\phin{\x}} \le \frac{\fe{L\abs{\x-\xnp}}-1}L \bnp \cdot Ch \bnl
    \implies & \bb{\x} = \phin{\x} + \bigabs{\bnp \cdot \frac{\fe{L\abs{\x-\xnp}}-1}L Ch}
  \end{align}
\end{proof}

\todo{Auswertung mit Fehler}

\section{Global version of de Moivre-Laplace}

In this section we will prove the statement:
\begin{align}
  \lim_{n\to\infty} \abs{\sum_{a \le \x \le b} \bb{\x[k]}- \int_a^b \fphi{t} \d{t}} = 0
\end{align}
and
\begin{align}
  \lim_{n\to\infty} \abs{\sum_{\x \le z} \bb{\x[k]} - \int_{-\infty}^z \fphi{t} \d{t}} = 0
\end{align}
for any $a,b,z\in\R$. Thereby we will split the proof in several steps. We start with the following lemma:

\begin{lemma} \label{alt:lemma1}
  For any $a,b \in \R$ with $a \le b$ we have

  \begin{align}
    \lim_{n\to\infty} \abs{\frac{\fphi{\xnp}h}{\bb{\xnp}} \sum_{a\le\x\le b} \bb{\x} - \int_a^b \fphi{t}\d{t}} = 0
  \end{align}
\end{lemma}

\begin{proof}
  From the local version of de Moivre-Laplace with $I=[a,b]$ we can deduce that there is a $C > 0$ independent of $\x$ and $n$ with (see remark \ref{remark:alt_local})
  \begin{align}
    \bb{\x} \in \frac{\bnp}{\fphi{\xnp}}\fphi{\x} + \bigabs{{\bnp}\cdot Ch} \subseteq \frac{\bnp}{\fphi{\xnp}}\fphi{\x} + \bnp \cdot \bigo{h}
  \end{align}
  With this equation follows
  \begin{align}
    \frac{\fphi{\xnp}h}{\bnp} \sum_{a \le \x \le b} \bb{\x} &\in \frac{\fphi{\xnp}h}{\bnp} \sum_{a \le \x \le b}\br{\frac{\bnp}{\fphi{\xnp}}\fphi{\x} + \bnp \cdot \bigo{h}} \bnl
    &
    \begin{comment}
      \lim_{n\to\infty} \xnp = \frac{1}{\sqrt{2\pi}} \implies \seq{\xnp} \in \bigo{1}
    \end{comment} \bnl
    &\subseteq \sum_{a \le \x \le b}\br{\fphi{\x}h + \bigo{h^2}}\bnl
    &
    \begin{comment}
      \begin{aligned}
        \sum_{a \le \x \le b} \fphi{\x}h &\in \int_a^b \fphi{t} \d{t} + \bigo{h} \bnl
        \sum_{a \le \x \le b} \bigo{h^2} &\subseteq \bigo{h}
      \end{aligned}
    \end{comment} \bnl
    &\subseteq \int_a^b \fphi{t} \d{t} + \bigo{h}
  \end{align}
\end{proof}

\begin{lemma}
  All $x\in\R$ with $0 < x \le \frac 12$ fulfill the inequality $\frac{1+x}{1-x} \le 1+4x$.
\end{lemma}

\begin{proof}
  \begin{align}
    \begin{array}{rrl}
      & x & \le \frac 12 \nl
      \implies & 4x^2 & \le 2x \nl
      \implies & 0 & \le  2x - 4x^2 \nl
      \implies & 1+x & \le  1+3x-4x^2 = (1+4x)(1-x) \nl
      &&
      \begin{comment}
        1-x > 0
      \end{comment} \nl
      \implies & \frac{1+x}{1-x} & \le 1+4x
    \end{array}
  \end{align}
\end{proof}

\begin{lemma}
  We find $\lim_{n\to\infty} \frac{\fphi{\xnp}h}{\bnp} = 1$.
\end{lemma}

\begin{remark}
  This lemma implies $\bnp \sim \frac{1}{\sqrt{2\pi npq}}$ because $\lim_{n\to\infty} \fphi{\xnp} = \sqrt{2\pi}$ as we already know from the standard proof of de Moivre-Laplace which bases on Stirling's formula. Therefore this lemma can be shown with Stirling's formula but I want to omit this approximation because it is only applicable for the binomial distribution and a use of Stirling's formula at this point contradicts the attempt to find a new proof of de Moivre-Laplace (which does not depend on methods of the standard proof).
\end{remark}

\begin{proof}
  Let $c_n = \frac{\fphi{\xnp}h}{\bnp}$. Let $\epsilon > 0$ be arbitrary with $\epsilon < 2$. From the Chebyshev's inequality we can get $\P{\abs{X}> R} \le \frac{1}{R^2}$ for any $R > 0$ and any standardized random variable $X$ \cite{wiki:chebyshev}. Thus there is a $R >0$ such that
  \begin{align}\label{eq1}
    1 -\frac{\epsilon}4 \le \sum_{-R \le \x \le R} \bb{\x[k]} \le 1
  \end{align}
  and
  \begin{align}\label{eq2}
    1-\frac{\epsilon}4 \le \int_{-R}^R \fphi{t}\d{t} \le 1
  \end{align}

  From the above lemma \ref{alt:lemma1} we know that there is a $N\in\N$ so that for each $n\in\N$ with $n\ge N$ we find
  \begin{align} \label{eq3}
    \abs{c_n \sum_{-R\le\x\le R}\bb{\x} - \int_{-R}^R \fphi{t}\d{t}} \le \frac{\epsilon}4
  \end{align}
  The inequalities \eqref{eq2} and \eqref{eq3} imply
  \begin{align}
    1-\frac{\epsilon}2 \le c_n \sum_{-R \le \x \le R} \bb{\x} \le 1+\frac{\epsilon}4
  \end{align}
  Together with the first inequality \eqref{eq1} we can conclude
  \begin{align}
    1-\frac \epsilon2 \le c_n \sum_{-R \le \x \le R} \bb{\x} \le c_n
  \end{align}
  and
  \begin{align}
    & 1+\frac \epsilon4 \ge c_n \sum_{-R \le \x \le R} \bb{\x} \ge c_n \br{1-\frac \epsilon4} \bnl
    \implies & c_n \le \frac{1+\frac \epsilon4}{1-\frac\epsilon4} \bnl
    &
    \begin{comment}
      0 < \frac \epsilon 4 \le \frac 12
    \end{comment} \bnl
    \implies & c_n \le 1+4\cdot \frac \epsilon 4 = 1+\epsilon
  \end{align}
  So we get
  \begin{align}
    1-\frac \epsilon 2 \le c_n \le 1+\epsilon
  \end{align}
  which proves $\abs{c_n-1} \le \epsilon$ and thus that $\lim_{n\to\infty} c_n = 1$.
\end{proof}

\begin{theorem}[First global version of de Moivre-Laplace]
  For any $a,b\in\R$ we find

  \begin{align}
    \lim_{n\to\infty} \abs{\sum_{a \le \x \le b} \bb{\x} - \int_{a \le t \le b} \fphi{t} \d{t} } = 0
  \end{align}
\end{theorem}

\begin{proof}
  For $a > b$ the theorem can be directly proven, because then
  \begin{align}
    \sum_{a \le \x \le b} \bb{\x} = \int_{a \le t \le b} \fphi{t} \d{t} = 0
  \end{align}
  So we consider only the case $a\le b$. In lemma \ref{alt:lemma1} we have already shown that
  \begin{align}
    \frac{\fphi{\xnp}h}{\bnp} \sum_{a\le \x \le b} \bb{\x} \in \int_a^b \fphi{t} \d{t} + \bigo{h}
  \end{align}
  Therefore
  \begin{align}
    \sum_{a\le \x \le b} \bb{\x} & \in \frac{\bnp}{\fphi{\xnp}h} \br{\int_a^b \fphi{t} \d{t} + \bigo{h}} \bnl
    &
    \begin{comment}
      \lim_{n\to\infty} \frac{\bnp}{\fphi{\xnp}h} = 1 \implies \frac{\bnp}{\fphi{\xnp}h} = 1+\littleo{1}
    \end{comment} \bnl
    & \subseteq \br{1+\littleo{1}}\cdot \br{\int_a^b \fphi{t} \d{t} + \bigo{h}} \bnl
    & = \int_a^b \fphi{t}\d{t} + \littleo{1} \cdot \int_a^b \fphi{t} \d{t} + \bigo{h} + \littleo{1} \cdot \bigo{h} \bnl
    &
    \begin{comment}
      \begin{aligned}
        \int_a^b \fphi{t} \d{t} & \le 1 \nl
        \bigo{h} & \subseteq \littleo{1}
      \end{aligned}
    \end{comment} \bnl
    & \subseteq \int_a^b \fphi{t} \d{t} + \littleo{1}
  \end{align}
\end{proof}

\begin{theorem}[Second global version of de Moivre-Laplace]
  For any $z\in\R$ we have

  \begin{align}
    \lim_{n\to\infty} \abs{\sum_{\x \le z} \bb{\x} - \int_{-\infty}^z \fphi{t}\d{t} } = 0
  \end{align}
\end{theorem}

\begin{proof}
  For any $c \le z$ we have the estimation

  \begin{multline}
    \abs{\sum_{\x \le z} \bb{\x} - \int_{-\infty}^z \fphi{t}\d{t} } \le
    \underbrace{\abs{\sum_{\x \le c} \bb{\x} - \int_{-\infty}^c \fphi{t}\d{t} }}_\text{term I} + \nl \underbrace{\abs{\sum_{c\le \x \le z} \bb{\x} - \int_c^z \fphi{t}\d{t} }}_\text{term II}
  \end{multline}

  First we take $c$ small enough such that term I is smaller than $\frac \epsilon 2$. Due to the Chebyshev's inequality \cite{wiki:chebyshev} we know that such a $c$ must exist. From the first global version of de Moivre-Laplace we know that there is a $N \in\N$ such that the term II is smaller then $\frac \epsilon 2$ for all $n\ge N$.
\end{proof}

\section{Conclusion}
